<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Primary Meta Tags -->
  <meta name="title"
    content="What Moves the Eyes: Doubling Mechanistic Model Performance Using Deep Networks to Discover and Test Cognitive Hypotheses - Federico D'Agostino, Lisa Schwetlick, Matthias Bethge, Matthias KÃ¼mmerer">
  <meta name="description"
    content="Bridging the gap between deep learning and mechanistic models for human scanpath prediction. Using DeepGaze III to discover cognitive mechanisms that double SceneWalk's performance from 35% to 70% explained variance.">
  <meta name="keywords"
    content="eye tracking, scanpath prediction, deep learning, mechanistic models, cognitive science, computer vision, neuroscience, visual attention">
  <meta name="author" content="Federico D'Agostino, Lisa Schwetlick, Matthias Bethge, Matthias KÃ¼mmerer">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Bethge Lab">
  <meta property="og:title"
    content="What Moves the Eyes: Doubling Mechanistic Model Performance Using Deep Networks to Discover and Test Cognitive Hypotheses">
  <meta property="og:description"
    content="Bridging the gap between deep learning and mechanistic models for human scanpath prediction. Using DeepGaze III to discover cognitive mechanisms that double SceneWalk's performance from 35% to 70% explained variance.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt"
    content="What Moves the Eyes: Doubling Mechanistic Model Performance Using Deep Networks to Discover and Test Cognitive Hypotheses - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="Matthias Kuemmerer">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@bethgelab">
  <meta name="twitter:creator" content="@fededagos">
  <meta name="twitter:title"
    content="What Moves the Eyes: Doubling Mechanistic Model Performance Using Deep Networks to Discover and Test Cognitive Hypotheses">
  <meta name="twitter:description"
    content="Bridging the gap between deep learning and mechanistic models for human scanpath prediction. Using DeepGaze III to discover cognitive mechanisms that double SceneWalk's performance from 35% to 70% explained variance.">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt"
    content="What Moves the Eyes: Doubling Mechanistic Model Performance Using Deep Networks to Discover and Test Cognitive Hypotheses - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title"
    content="What Moves the Eyes: Doubling Mechanistic Model Performance Using Deep Networks to Discover and Test Cognitive Hypotheses">
  <meta name="citation_author" content="D'Agostino, Federico">
  <meta name="citation_author" content="Schwetlick, Lisa">
  <meta name="citation_author" content="Bethge, Matthias">
  <meta name="citation_author" content="KÃ¼mmerer, Matthias">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="NeurIPS 2025">
  <!-- TODO: Replace with your paper PDF URL -->
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">

  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">

  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>What Moves the Eyes: Doubling Mechanistic Model Performance Using Deep Networks to Discover and Test Cognitive
    Hypotheses - Federico D'Agostino, Lisa Schwetlick, Matthias Bethge, Matthias KÃ¼mmerer | Academic Research</title>

  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">

  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">

  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>

  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>

  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "What Moves the Eyes: Doubling Mechanistic Model Performance Using Deep Networks to Discover and Test Cognitive Hypotheses",
    "description": "Bridging the gap between deep learning and mechanistic models for human scanpath prediction. Using DeepGaze III to discover cognitive mechanisms that double SceneWalk's performance from 35% to 70% explained variance.",
    "author": [
      {
        "@type": "Person",
        "name": "Federico D'Agostino",
        "affiliation": {
          "@type": "Organization",
          "name": "TÃ¼bingen AI Center, University of TÃ¼bingen"
        }
      },
      {
        "@type": "Person",
        "name": "Lisa Schwetlick",
        "affiliation": {
          "@type": "Organization",
          "name": "EPFL"
        }
      },
      {
        "@type": "Person",
        "name": "Matthias Bethge *",
        "affiliation": {
          "@type": "Organization",
          "name": "TÃ¼bingen AI Center, University of TÃ¼bingen"
        }
      },
      {
        "@type": "Person",
        "name": "Matthias KÃ¼mmerer *",
        "affiliation": {
          "@type": "Organization",
          "name": "TÃ¼bingen AI Center, University of TÃ¼bingen"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "NeurIPS 2025"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["eye tracking", "scanpath prediction", "deep learning", "mechanistic models", "cognitive science", "computer vision", "neuroscience", "visual attention"],
    "abstract": "Understanding how humans move their eyes to gather visual information is a central question in neuroscience, cognitive science, and vision research. While recent deep learning (DL) models achieve state-of-the-art performance in predicting human scanpaths, their underlying decision processes remain opaque. At an opposite end of the modeling spectrum, cognitively inspired mechanistic models aim to explain scanpath behavior through interpretable cognitive mechanisms but lag far behind in predictive accuracy. In this work, we bridge this gap by using a high-performing deep modelâ€”DeepGaze IIIâ€”to discover and test mechanisms that improve a leading mechanistic model, SceneWalk. By identifying individual fixations where DeepGaze III succeeds and SceneWalk fails, we isolate behaviorally meaningful discrepancies and use them to motivate targeted extensions of the mechanistic framework. These include time-dependent temperature scaling, saccadic momentum and an adaptive cardinal attention bias: Simple, interpretable additions that substantially boost predictive performance. With these extensions, SceneWalk's explained variance on the MIT1003 dataset doubles from 35% to 70%, setting a new state of the art in mechanistic scanpath prediction. Our findings show how performance-optimized neural networks can serve as tools for cognitive model discovery, offering a new path toward interpretable and high-performing models of visual behavior.",
    "citation": "@article{dagostino2024what,
  title={What Moves the Eyes: Doubling Mechanistic Model Performance Using Deep Networks to Discover and Test Cognitive Hypotheses},
  author={D'Agostino, Federico and Schwetlick, Lisa and Bethge, Matthias and KÃ¼mmerer, Matthias},
  journal={NeurIPS},
  year={2025}
}",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Eye Tracking"
      },
      {
        "@type": "Thing", 
        "name": "Cognitive Science"
      },
      {
        "@type": "Thing",
        "name": "Computer Vision"
      },
      {
        "@type": "Thing",
        "name": "Neuroscience"
      }
    ]
  }
  </script>

  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Bethge Lab",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>

<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">What Moves the Eyes: Doubling Mechanistic Model Performance Using
                Deep Networks to Discover and Test Cognitive Hypotheses</h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="mailto:federico.dagostino@bethgelab.org" target="_blank">Federico D'Agostino</a>,</span>
                <span class="author-block">
                  <a href="mailto:lisa.schwetlick@epfl.ch" target="_blank">Lisa Schwetlick</a>,</span>
                <span class="author-block">
                  <a href="mailto:matthias@bethgelab.org" target="_blank">Matthias Bethge</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="mailto:matthias.kuemmerer@bethgelab.org" target="_blank">Matthias KÃ¼mmerer</a><sup>*</sup>
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block">TÃ¼bingen AI Center, University of TÃ¼bingen<br>EPFL</span>
                <span class="eql-cntrb"><small><br><sup>*</sup>Shared Senior Authors</small></span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">

                  <span class="link-block">
                    <a href="https://openreview.net/pdf?id=cnrwqlr8dg" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://github.com/bethgelab/what-moves-the-eyes" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <!-- TODO: Update with your arXiv paper ID -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <!-- TODO: Remove coming soon once added -->
                      <span>arXiv (coming soon)</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>


    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Understanding how humans move their eyes to gather visual information is a central question in
                neuroscience, cognitive science, and vision research. While recent deep learning (DL) models achieve
                state-of-the-art performance in predicting human scanpaths, their underlying decision processes remain
                opaque. At an opposite end of the modeling spectrum, cognitively inspired mechanistic models aim to
                explain scanpath behavior through interpretable cognitive mechanisms but lag far behind in predictive
                accuracy. In this work, we bridge this gap by using a high-performing deep model - DeepGaze III - to
                discover and test mechanisms that improve a leading mechanistic model, SceneWalk. By identifying
                individual fixations where DeepGaze III succeeds and SceneWalk fails, we isolate behaviorally meaningful
                discrepancies and use them to motivate targeted extensions of the mechanistic framework. These include
                time-dependent temperature scaling, saccadic momentum and an adaptive cardinal attention bias: Simple,
                interpretable additions that substantially boost predictive performance. With these extensions,
                SceneWalk's explained variance on the MIT1003 dataset doubles from 35% to 70%, setting a new state of
                the art in mechanistic scanpath prediction. Our findings show how performance-optimized neural networks
                can serve as tools for cognitive model discovery, offering a new path toward interpretable and
                high-performing models of visual behavior.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->

    <!-- TL;DR -->
    <section class="section hero is-small">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">TL;DR</h2>
            <div class="content has-text-justified">
              <p>
                <strong>A systematic fixation-level comparison of a performance-optimized DNN scanpath model and a
                  mechanistic
                  cognitive model reveals behaviourally relevant mechanisms that can be added to the mechanistic model
                  to
                  substantially improve performance.</strong>
              </p>
              <br>
              <br>
              <div class="has-text-centered">
                <figure>
                  <img src="static/images/figure_1.png" alt="Overview" loading="lazy" style="max-width: 100%;" />
                  <figcaption>
                    (a) We systematically compare prediction performances of a mechanistic scanpath model
                    (SceneWalk) to a high-performing DNN model (DeepGaze III) to find situations that could be
                    predicted very well but are not by the mechanistic model. Inspecting these extreme cases results
                    in ideas for effects than can be confirmed through further analyses and give rise to mechanisms
                    that are added to the mechanistic model. (b) This process yields three new mechanisms that double
                    SceneWalk's predictive performance on MIT1003, substantially narrowing the explainability gap.
                  </figcaption>
                </figure>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End TL;DR -->


    <!-- Youtube video -->
    <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container is-centered has-text-centered">
          <!-- Paper video. -->
          <h2 class="title is-3">Video Presentation</h2>
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <div class="publication-video">
                <iframe src="https://www.youtube.com/embed/Po8PhnWAWSY?si=qtlLm8jNKlBEdEEH" frameborder="0"
                  allow="autoplay; encrypted-media" allowfullscreen
                  referrerpolicy="strict-origin-when-cross-origin"></iframe>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End youtube video -->


    <!-- Introduction -->
    <section class="section hero is-small">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Introduction</h2>
            <div class="content has-text-justified">

              <p>
                Science often faces a choice:
                <br>
                Build models primarily designed to predict, or models that compactly explain. But what if we used them
                in
                synergy?
                <br>
                <br>

                Our paper tackles this head-on. We combine a deep network (DeepGaze III) with an interpretable
                mechanistic
                model (SceneWalk).
                <br>
                <br>

                ðŸ’¡ Our idea: Use the deep model not just to chase performance, but as a tool for scientific discovery.
                <br>
                We isolate "controversial fixations" where DeepGaze's likelihood vastly exceeds SceneWalk's.
                These reveal where the mechanistic model fails to capture predictable patterns.

              </p>
              <br>

              <div class="has-text-centered">
                <figure>
                  <img src="static/images/figure_2.png" alt="Controversial fixations" loading="lazy"
                    style="max-width: 100%;" />
                  <figcaption>
                    Controversial Fixations: Those fixations in the dataset where SceneWalk loses most
                    performance compared to DeepGaze III in terms of LLD or WLLD. Numbers on top of predictions
                    indicate the achieved log-likelihood relative to a uniform model for the fixation in question.
                  </figcaption>
                </figure>
              </div>
              <br>
              <br>
              <p>
                From these systematic failures, we isolated three critical mechanisms SceneWalk was missing.
                The data pointed to known cognitive principles, but revealed critical new nuances.
                Our method showed us not just what was missing, but how to formulate it to match human behavior. ðŸ‘‡
              </p>
            </div>
            <br>

          </div>
        </div>
      </div>
      </div>
    </section>
    <!--End introduction -->


    <!-- Image carousel -->
    <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container is-centered has-text-centered">
          <h2 class="title is-3">New mechanisms</h2>
          <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <img src="static/images/figure_3.png" alt="Time-dependent temperature scaling" loading="lazy"
                  style="max-width: 60%;" />
                <h2 class="subtitle has-text-centered">
                  Time dependent temperature scaling. We found that DeepGaze shows higher confidence
                  (less entropy) early on (a) and predicts fixations in more salient locations (b) in agreement with the
                  empirical data. SceneWalk is not showing this effect. To address this, we introduced fixation-index-
                  dependent temperature scaling (c), modeled with exponential decay (d), which improves predictions
                  for both early and late fixations (e).
                </h2>
              </div>
              <div class="item">
                <!-- Your image here -->
                <img src="static/images/figure_4.png" alt="Saccadic momentum" loading="lazy" style="max-width: 60%;" />
                <h2 class="subtitle has-text-centered">
                  Saccadic momentum: Controversial fixations suggested that DeepGaze sometimes prefers
                  saccades to continue in the same direction. We confirmed such a saccadic momentum effect especially
                  after long saccades (a) and later in scanpaths (b). These effects remain even after controlling for
                  the distribution of salient objects (dashed lines), indicating a genuine directional bias. We added a
                  saccadic momentum and return mechanism to SceneWalk (c) modulated by previous saccade length
                  (d) and fixation index (e), improving predictions for "ongoing saccades" controversial fixations (f).
                </h2>
              </div>
              <div class="item">
                <!-- Your image here -->
                <img src="static/images/figure_5.png" alt="Cardinal attention bias" loading="lazy"
                  style="max-width: 60%;" />
                <h2 class="subtitle has-text-centered">
                  Horizontal and left-wards attention bias: Controversial fixations suggested that DeepGaze
                  predicts early saccades to go to the left. This is indeed the case in the data (a) and the DeepGaze
                  predictions (b). The effect persists when DeepGaze is run with uniform saliency maps (c) while
                  SceneWalk (d) shows a too weak effect that increases over time. Therefore, we added a cardinal
                  attention bias to SceneWalk, than can additionally have a left-asymmetry (e) and can adapt over time
                  (f, g). This improves predictions on the relevant controversial fixations (f).
                </h2>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End image carousel -->

    <!-- Results -->
    <section class="section hero is-small">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Results</h2>
            <div class="content has-text-justified">
              <p>
                These 3 mechanisms double SceneWalk's explained variance on the MIT1003 dataset (from 35 % â†’ 70 %)! We
                closed over 56 % of the gap to deep networks, setting a new State-of-the-Art for mechanistic scanpath
                prediction.

                <br>

                Conceptually: Deep neural networks should be viewed as scientific instruments. They tell us what is
                predictable in human behavior.

                We then use that information to ask why, building fully interpretable models that approach the
                performance of their black-box counterparts.
              </p>
              <br>
              <br>
              <div class="has-text-centered">
                <figure>
                  <img src="static/images/figure_6.png" alt="Results" loading="lazy" style="max-width: 100%;" />
                  <figcaption>
                    Contributions of the different added mechanisms on the MIT1003 and DAEMONS datasets.
                  </figcaption>
                </figure>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End Results -->



    <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code>@article{dagostino2024what,
  title={What Moves the Eyes: Doubling Mechanistic Model Performance Using Deep Networks to Discover and Test Cognitive Hypotheses},
  author={D'Agostino, Federico and Schwetlick, Lisa and Bethge, Matthias and KÃ¼mmerer, Matthias},
  journal={NeurIPS},
  year={2025}
}</code></pre>
      </div>
    </section>
    <!--End BibTex citation -->


    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">

              <p>
                This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a
                  href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
                You are free to borrow the source code of this website, we just ask that you link back to this page in
                the footer. <br> This website is licensed under a <a rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                  Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>

            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>

</html>
